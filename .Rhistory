# Run k-means on training data
set.seed(42)
km_model <- kmeans(X_train_pca, centers = k_optimal,
nstart = 25, iter.max = 300, algorithm = "Lloyd")
# Evaluate clusters
table(km_model$cluster)
# Ensure X_train_pca is a dataframe and add cluster labels
X_train_pca_df <- as.data.frame(X_train_pca)
X_train_pca_df$cluster <- factor(km_model$cluster)
# Create a scatter plot of clusters using PC1 and PC2 (training set only)
ggplot(data = X_train_pca_df, aes(x = PC1, y = PC2, color = cluster)) +
geom_point(alpha = 0.5) +
labs(title = "Cluster Visualization (PCA - training set)", x = "PC1", y = "PC2") +
theme_minimal()
ggplot(data = data.frame(cluster = km_model$cluster, y_train = y_train), aes(x = factor(cluster), y = y_train)) +
geom_boxplot() +
labs(title = "Distribution of Target Variable by Cluster", x = "Cluster", y = "Target Variable")
# Mean of the target column
y_mean <- mean(y_train)
# Forecast validation and test sets using mean price
val_forecast_baseline <- rep(y_mean, length(y_val))
test_forecast_baseline <- rep(y_mean, length(y_test))
# Mean Absolute Error (MAE) for validation and test sets
val_mae_baseline <- mean(abs(y_val - val_forecast_baseline))
test_mae_baseline <- mean(abs(y_test - test_forecast_baseline))
# Print results
cat("Mean price from training set:", y_mean, "\n")
cat("Validation MAE (Baseline):", val_mae_baseline, "\n")
cat("Test MAE (Baseline):", test_mae_baseline, "\n")
X_train <- model.matrix(~ ., data = X_train)[, -1]
X_val <- model.matrix(~ ., data = X_val)[, -1]
X_test <- model.matrix(~ ., data = X_test)[, -1]
# Fit the model
model_ridge <- glmnet(X_train, y_train, alpha = 0)
# # Plot coefficient shrinkage across lambda values
# plot(model_ridge, xvar = "lambda")
# Cross-validation for optimal lambda
cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0)
# # Plot cross-validation results
# plot(cv_ridge)
# Optimal lambda value
lambda_optimal <- cv_ridge$lambda.min
cat("Optimal lambda:", lambda_optimal, "\n")
# Refit model with optimal lambda
model_ridge_optimal <- glmnet(X_train, y_train, alpha = 0, lambda = lambda_optimal)
# Evaluation
val_predictions_ridge <- predict(model_ridge_optimal,
s = lambda_optimal, newx = X_val)
test_predictions_ridge <- predict(model_ridge_optimal,
s = lambda_optimal, newx = X_test)
# MAE for validation and test sets
val_mae_ridge <- mean(abs(y_val - val_predictions_ridge))
test_mae_ridge <- mean(abs(y_test - test_predictions_ridge))
cat("Validation MAE (Ridge Regression):", val_mae_ridge, "\n")
cat("Validation MAE (Ridge Regression):", test_mae_ridge, "\n")
# # Plot predictions vs actual values (test set)
# plot(test_predictions_ridge, y_test)
# abline(0, 1)
# Plot coefficient shrinkage across lambda values
plot(model_ridge, xvar = "lambda")
# Plot cross-validation results
plot(cv_ridge)
# Plot predictions vs actual values (test set)
plot(test_predictions_ridge, y_test)
abline(0, 1)
# Fit the model
model_lasso <- glmnet(X_train, y_train, alpha = 1)  # Set alpha = 1 for LASSO
# # Plot coefficient shrinkage across lambda values
# plot(model_lasso, xvar = "lambda")
# Cross-validation for optimal lambda
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
# # Plot cross-validation results
# plot(cv_lasso)
# Optimal lambda value
lambda_optimal <- cv_lasso$lambda.min
cat("Optimal lambda:", lambda_optimal, "\n")
# Refit model with optimal lambda
model_lasso_optimal <- glmnet(X_train, y_train, alpha = 1, lambda = lambda_optimal)
# Evaluation
val_predictions_lasso <- predict(model_lasso_optimal,
s = lambda_optimal, newx = X_val)
test_predictions_lasso <- predict(model_lasso_optimal,
s = lambda_optimal, newx = X_test)
# MAE for validation and test sets
val_mae_lasso <- mean(abs(y_val - val_predictions_lasso))
test_mae_lasso <- mean(abs(y_test - test_predictions_lasso))
cat("Validation MAE (Lasso Regression):", val_mae_lasso, "\n")
cat("Test MAE (Lasso Regression):", test_mae_lasso, "\n")
# # Plot predictions vs actual values (test set)
# plot(test_predictions_lasso, y_test)
# abline(0, 1)
# Plot coefficient shrinkage across lambda values
plot(model_lasso, xvar = "lambda")
# Plot cross-validation results
plot(cv_lasso)
# Plot predictions vs actual values (test set)
plot(test_predictions_lasso, y_test)
abline(0, 1)
# Training
model_rf <- randomForest(x = X_train, y = y_train, ntree = 443,
mtry = 13, importance = TRUE)
print(model_rf)
# Evaluate variables
# varImpPlot(model_rf) # Plot variable importance
importance(model_rf) # Display importance metrics
# Predict and evaluate model performance
val_predictions_rf <- predict(model_rf, newdata = X_val)
test_predictions_rf <- predict(model_rf, newdata = X_test)
# MAE for validation and test sets
val_mae_rf <- mean(abs(y_val - val_predictions_rf))
test_mae_rf <- mean(abs(y_test - test_predictions_rf))
cat("Validation MAE (Random Forest):", val_mae_rf, "\n")
cat("Test MAE (Random Forest):", test_mae_rf, "\n")
# # Plot actual vs predicted values
# plot(y_test, test_predictions_rf)
# abline(0, 1) # Add a diagonal line for reference
# Plot actual vs predicted values
plot(y_test, test_predictions_rf)
abline(0, 1) # Add a diagonal line for reference
# Standardization
X_train <- scale(X_train)
# Use training set's mean and standard deviation for scaling validation and test sets
X_val <- scale(X_val, center = attr(X_train, "scaled:center"),
scale = attr(X_train, "scaled:scale"))
X_test <- scale(X_test, center = attr(X_train, "scaled:center"),
scale = attr(X_train, "scaled:scale"))
tf$random$set_seed(42) # Set seed for tf
# Define the model with dropout layers
model_fnn <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 1)
# Compile the model
model_fnn %>% compile(
optimizer = "adam",
loss = "mse",
metrics = c("mae")
)
# Early stopping
early_stopping <- callback_early_stopping(
monitor = "val_loss",
patience = 10,
restore_best_weights = TRUE
)
# Train the model
history <- model_fnn %>% fit(
x = as.matrix(X_train),
y = y_train,
validation_data = list(as.matrix(X_val), y_val),
epochs = 100,
batch_size = 32,
verbose = 0,
callbacks = list(early_stopping)
)
# Plot training history
plot(history)
# Identify the chosen epoch with the lowest validation loss
best_epoch <- which.min(history$metrics$val_loss) # Index of minimum validation loss
cat("Best epoch (with lowest val_loss):", best_epoch, "\n")
cat("Validation loss at best epoch:", min(history$metrics$val_loss), "\n")
# Evaluate
model_fnn %>% evaluate(as.matrix(X_val), y_val)
model_fnn %>% evaluate(as.matrix(X_test), y_test)
# Standardization
X_train <- scale(X_train)
# Use training set's mean and standard deviation for scaling validation and test sets
X_val <- scale(X_val, center = attr(X_train, "scaled:center"),
scale = attr(X_train, "scaled:scale"))
X_test <- scale(X_test, center = attr(X_train, "scaled:center"),
scale = attr(X_train, "scaled:scale"))
tf$random$set_seed(42) # Set seed for tf
# Define the model with dropout layers
model_fnn <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 1)
# Compile the model
model_fnn %>% compile(
optimizer = "adam",
loss = "mse",
metrics = c("mae")
)
# Early stopping
early_stopping <- callback_early_stopping(
monitor = "val_loss",
patience = 10,
restore_best_weights = TRUE
)
# Train the model
history <- model_fnn %>% fit(
x = as.matrix(X_train),
y = y_train,
validation_data = list(as.matrix(X_val), y_val),
epochs = 100,
batch_size = 32,
verbose = 0,
callbacks = list(early_stopping)
)
# Plot training history
plot(history)
# Identify the chosen epoch with the lowest validation loss
best_epoch <- which.min(history$metrics$val_loss) # Index of minimum validation loss
cat("Best epoch (with lowest val_loss):", best_epoch, "\n")
cat("Validation loss at best epoch:", min(history$metrics$val_loss), "\n")
# Evaluate
model_fnn %>% evaluate(as.matrix(X_val), y_val)
model_fnn %>% evaluate(as.matrix(X_test), y_test)
View(X_train)
View(X_train)
# Import Python libraries
library(reticulate)
use_condaenv("ec349_env", required = TRUE)
library(tensorflow)
library(keras)
library(plotly)
# Import R packages
# install.packages("pacman")
library(pacman)
pacman::p_load(pacman, dplyr, ggplot2, plotly, tidyr, readr, plotly,
caret, glmnet, randomForest, ParBayesianOptimization,
progressr)
# Global progress bar
handlers("txtprogressbar")
data <- read_csv("listings.csv")
# Keep relevant columns
data_cleaned <- data %>% select(c(price,
room_type, property_type, accommodates,
bathrooms, bedrooms, beds, amenities,
neighbourhood_cleansed,
latitude, longitude, review_scores_location,
host_is_superhost, host_since,
host_response_rate,
calculated_host_listings_count,
host_has_profile_pic,
host_identity_verified,
review_scores_rating, number_of_reviews,
availability_30, availability_365))
# Replace blank values with NA
data_cleaned[data_cleaned == ""] <- NA
# Drop missing values in the 'price' column
data_cleaned <- data_cleaned %>% drop_na(price)
# # Check columns numeric
# sapply(data_cleaned, is.numeric)
# Remove '$' sign in the 'price' column
data_cleaned$price <- as.numeric(gsub("[$,]", "", data_cleaned$price))
# Replace "N/A" with NA in 'host_response_rate'
data_cleaned$host_response_rate[data_cleaned$host_response_rate == "N/A"] <- NA
# Remove the "%" symbol and convert to decimals in 'host_response_rate'
data_cleaned$host_response_rate <-
as.numeric(gsub("%", "", data_cleaned$host_response_rate))/100
# Convert TRUE/FALSE into 1 or 0
data_cleaned <- data_cleaned %>%
mutate_at(vars(host_is_superhost, host_has_profile_pic,
host_identity_verified), as.numeric)
# Convert the 'host_since' column to date format
data_cleaned$host_since <- as.Date(data_cleaned$host_since)
# Add 'host_years_of_experience' column and drop the 'host_since' column
data_cleaned$host_years_of_experience <-
as.numeric(difftime("2024-12-11",
data_cleaned$host_since,
units = "days")) / 365
data_cleaned <- data_cleaned %>% select(-host_since)
# Drop 'review_scores_rating', 'review_scores_location'
# and 'host_response_rate' due to large numbers of missing values
data_cleaned <- data_cleaned %>% select(-c(review_scores_rating,
review_scores_location,
host_response_rate))
# Drop extreme outliers
data_cleaned <- data_cleaned %>% filter(price <= 1000)
# Replace "[]" with NA in the 'amenities' column
data_cleaned$amenities[data_cleaned$amenities == "[]"] <- NA
# # Check for the number of missing values in each column
# colSums(is.na(data_cleaned))
# # Check the number of unique values in 'amenities'
# n_distinct(data_cleaned$amenities)
# Drop the 'amenities' column
data_cleaned <- data_cleaned %>% select(-amenities)
# Drop missing values
data_cleaned <- data_cleaned %>% drop_na()
set.seed(42) # For reproducibility
# Define train, val, test sizes
train_size <- 0.7
val_size <- 0.2
test_size <- 0.1
n <- nrow(data_cleaned) # Total number of rows
indices <- sample(seq_len(n)) # Shuffle row indices
# Compute split indices
train_indices <- indices[1:floor(train_size * n)]
val_indices <-
indices[(floor(train_size * n) + 1):(floor((train_size + val_size) * n))]
test_indices <- indices[(floor((train_size + val_size) * n) + 1):n]
# One-hot encoding
dummy <- dummyVars(" ~ .", data = data_cleaned, levelsOnly = FALSE)
data_encoded <- data.frame(predict(dummy, newdata = data_cleaned))
# Remove predictors with zero variance
zero_variance_cols <- nearZeroVar(data_encoded)
data_encoded <- data_encoded[, -zero_variance_cols]
# Split data into train, validation, and test sets
target <- 'price'
y <- data_encoded %>% pull(target)
X <- data_encoded %>% select(-target)
X_train <- X[train_indices, , drop = FALSE]
y_train <- y[train_indices]
X_val <- X[val_indices, , drop = FALSE]
y_val <- y[val_indices]
X_test <- X[test_indices, , drop = FALSE]
y_test <- y[test_indices]
# Print dimensions
cat("X_train shape:", dim(X_train), "\n")
cat("y_train shape:", length(y_train), "\n")
cat("X_val shape:", dim(X_val), "\n")
cat("y_val shape:", length(y_val), "\n")
cat("X_test shape:", dim(X_test), "\n")
cat("y_test shape:", length(y_test), "\n")
# Training
model_rf <- randomForest(x = X_train, y = y_train, ntree = 5,
mtry = 4, importance = TRUE)
print(model_rf)
# Evaluate variables
importance(model_rf) # Display importance metrics
# Predict and evaluate model performance
val_predictions_rf <- predict(model_rf, newdata = X_val)
test_predictions_rf <- predict(model_rf, newdata = X_test)
# MAE for validation and test sets
val_mae_rf <- mean(abs(y_val - val_predictions_rf))
test_mae_rf <- mean(abs(y_test - test_predictions_rf))
cat("Validation MAE (Random Forest):", val_mae_rf, "\n")
cat("Test MAE (Random Forest):", test_mae_rf, "\n")
# Plot variable importance based on %IncMSE
importance <- as.data.frame(varImp(model_rf)) # Extract importance values
importance$Variable <- rownames(importance) # Add variable names
# Sort by importance and plot top 10 variables
ggplot(importance, aes(x = reorder(Variable, Overall), y = Overall)) +
geom_bar(stat = "identity") +
coord_flip() +
theme_minimal() +
theme(axis.text.y = element_text(size = 7)) + # Adjust text size
labs(x = "Variables", y = "Importance", title = "Variable Importance")
# Import Python libraries
library(reticulate)
use_condaenv("ec349_env", required = TRUE)
library(tensorflow)
library(keras)
library(plotly)
# Import R packages
# install.packages("pacman")
library(pacman)
pacman::p_load(pacman, dplyr, ggplot2, plotly, tidyr, readr, plotly,
caret, glmnet, randomForest, ParBayesianOptimization,
progressr)
# Global progress bar
handlers("txtprogressbar")
# Import the dataset
data <- read_csv("listings.csv")
# Keep relevant columns
data_cleaned <- data %>% select(c(price,
room_type, property_type, accommodates,
bathrooms, bedrooms, beds, amenities,
neighbourhood_cleansed,
latitude, longitude, review_scores_location,
host_is_superhost, host_since,
host_response_rate,
calculated_host_listings_count,
host_has_profile_pic,
host_identity_verified,
review_scores_rating, number_of_reviews,
availability_30, availability_365))
# Replace blank values with NA
data_cleaned[data_cleaned == ""] <- NA
# Drop missing values in the 'price' column
data_cleaned <- data_cleaned %>% drop_na(price)
# # Check columns numeric
# sapply(data_cleaned, is.numeric)
# Remove '$' sign in the 'price' column
data_cleaned$price <- as.numeric(gsub("[$,]", "", data_cleaned$price))
# Replace "N/A" with NA in 'host_response_rate'
data_cleaned$host_response_rate[data_cleaned$host_response_rate == "N/A"] <- NA
# Remove the "%" symbol and convert to decimals in 'host_response_rate'
data_cleaned$host_response_rate <-
as.numeric(gsub("%", "", data_cleaned$host_response_rate))/100
# Convert TRUE/FALSE into 1 or 0
data_cleaned <- data_cleaned %>%
mutate_at(vars(host_is_superhost, host_has_profile_pic,
host_identity_verified), as.numeric)
# Convert the 'host_since' column to date format
data_cleaned$host_since <- as.Date(data_cleaned$host_since)
# Add 'host_years_of_experience' column and drop the 'host_since' column
data_cleaned$host_years_of_experience <-
as.numeric(difftime("2024-12-11",
data_cleaned$host_since,
units = "days")) / 365
data_cleaned <- data_cleaned %>% select(-host_since)
# Drop 'review_scores_rating', 'review_scores_location'
# and 'host_response_rate' due to large numbers of missing values
data_cleaned <- data_cleaned %>% select(-c(review_scores_rating,
review_scores_location,
host_response_rate))
# Drop extreme outliers
data_cleaned <- data_cleaned %>% filter(price <= 1000)
# Replace "[]" with NA in the 'amenities' column
data_cleaned$amenities[data_cleaned$amenities == "[]"] <- NA
# # Check for the number of missing values in each column
# colSums(is.na(data_cleaned))
# # Check the number of unique values in 'amenities'
# n_distinct(data_cleaned$amenities)
# Drop the 'amenities' column
data_cleaned <- data_cleaned %>% select(-amenities)
# Drop missing values
data_cleaned <- data_cleaned %>% drop_na()
# Boxplot for 'price'
ggplot(data_cleaned, aes(x = price)) +
geom_boxplot(fill = "blue", color = "black") +
labs(title = "Boxplot of Price", x = "Price")
# Frequency of values in 'room_type'
table(data_cleaned$room_type)
# Plot bar chart for 'room_type'
ggplot(data_cleaned, aes(x = room_type)) +
geom_bar(fill = "blue", color = "black") +
labs(title = "Bar Chart of Room Type", x = "Room Type", y = "Frequency")
data_Entire_home_apt <- data_cleaned %>% filter(room_type == "Entire home/apt")
# Mean price by 'neighbourhood_cleansed' for Entire home/apt
mean_price_by_location <- data_Entire_home_apt %>%
group_by(neighbourhood_cleansed) %>%
summarize(mean_price = mean(price, na.rm = TRUE)) %>%
arrange(desc(mean_price))
# Plot the mean price by neighbourhood location
ggplot(mean_price_by_location, aes(x = reorder(neighbourhood_cleansed,
-mean_price),
y = mean_price)) +
geom_bar(stat = "identity", fill = "blue") +
labs(title = "Mean Price by Neighbourhood (Entire home/apt)",
x = "Neighbourhood",
y = "Mean Price") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Select relevant columns
data_subset <- data_cleaned %>%
select(price, accommodates, bathrooms, bedrooms, beds, host_is_superhost,
calculated_host_listings_count, host_has_profile_pic,
host_identity_verified,
number_of_reviews, availability_30, availability_365,
host_years_of_experience)
# Reshape data into long format
data_subset_long <- pivot_longer(data_subset, cols = -price,
names_to = "Variable", values_to = "Value")
# Create scatter plots with faceting
ggplot(data_subset_long, aes(x = Value, y = price)) +
geom_point(alpha = 0.5, color = "blue") +
facet_wrap(~Variable, scales = "free_x") + # Separate plots for each variable
labs(title = "Scatter Plots of Price vs Other Variables",
x = "Variable Values",
y = "Price (USD)") +
theme_minimal()
set.seed(42) # For reproducibility
# Define train, val, test sizes
train_size <- 0.7
val_size <- 0.2
test_size <- 0.1
n <- nrow(data_cleaned) # Total number of rows
indices <- sample(seq_len(n)) # Shuffle row indices
# Compute split indices
train_indices <- indices[1:floor(train_size * n)]
val_indices <-
indices[(floor(train_size * n) + 1):(floor((train_size + val_size) * n))]
test_indices <- indices[(floor((train_size + val_size) * n) + 1):n]
# One-hot encoding
dummy <- dummyVars(" ~ .", data = data_cleaned, levelsOnly = FALSE)
data_encoded <- data.frame(predict(dummy, newdata = data_cleaned))
# Remove predictors with zero variance
zero_variance_cols <- nearZeroVar(data_encoded)
data_encoded <- data_encoded[, -zero_variance_cols]
# Split data into train, validation, and test sets
target <- 'price'
y <- data_encoded %>% pull(target)
X <- data_encoded %>% select(-target)
X_train <- X[train_indices, , drop = FALSE]
y_train <- y[train_indices]
X_val <- X[val_indices, , drop = FALSE]
y_val <- y[val_indices]
X_test <- X[test_indices, , drop = FALSE]
y_test <- y[test_indices]
# Print dimensions
cat("X_train shape:", dim(X_train), "\n")
cat("y_train shape:", length(y_train), "\n")
cat("X_val shape:", dim(X_val), "\n")
cat("y_val shape:", length(y_val), "\n")
cat("X_test shape:", dim(X_test), "\n")
cat("y_test shape:", length(y_test), "\n")
# Standardization (only on training set)
X_train_scaled <- scale(X_train)
# Check dimensions
dim(X_train_scaled)
# Reduce dimensionality (PCA) before clustering (keep ~95% variance)
pca_result <- prcomp(X_train_scaled)
num_components <-
which(cumsum(pca_result$sdev^2) / sum(pca_result$sdev^2) >= 0.95)[1]
X_train_pca <- pca_result$x[, 1:num_components]
# Check dimensions
dim(X_train_pca)
# Determine optimal number of clusters (elbow method)
# Compute Within-Cluster Sum of Squares (WCSS) for k = 1 to 20 (on training data)
wss <- numeric(20)
for (k in 1:20) {
km_model <- kmeans(X_train_pca, centers = k,
nstart = 10,
iter.max = 300,
algorithm = "Lloyd")
wss[k] <- km_model$tot.withinss
}
